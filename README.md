# ğŸ“š Campus Knowledge Assistant

A powerful **Retrieval-Augmented Generation (RAG)** system built with LangChain, Groq, and Streamlit that allows you to chat with your PDF documents using advanced AI. Perfect for students, researchers, and professionals who want to extract insights from their documents through natural conversation.

## âœ¨ Features

### ğŸ¤– AI-Powered Q&A
- **Conversational Memory**: Remembers previous questions and answers for contextual follow-ups
- **Smart Question Rephrasing**: Automatically understands references like "explain its types" from previous context
- **Source Citations**: Shows exactly which documents and pages were used for each answer

### ğŸ“„ Document Processing
- **Multi-PDF Support**: Upload and process multiple PDF documents simultaneously
- **Intelligent Chunking**: Splits documents into optimal 1000-character chunks with 200-character overlap
- **Metadata Preservation**: Maintains source filenames and page numbers for accurate citations

### ğŸ” Advanced Search
- **Semantic Search**: Uses vector embeddings for meaning-based retrieval (not just keyword matching)
- **Local Embeddings**: Privacy-focused - no data sent to external embedding services
- **Configurable Retrieval**: Adjustable number of relevant chunks (k=4 by default)

### ğŸ¨ User Interfaces
- **Web App (Streamlit)**: Modern, intuitive web interface with drag-and-drop PDF uploads
- **Command-Line Tool**: Lightweight terminal interface for quick queries
- **Real-time Chat**: Interactive conversation with instant responses

### ğŸ› ï¸ Technical Stack
- **LLM**: Groq's Llama 3.1 8B Instant (fast, cost-effective inference)
- **Embeddings**: Sentence Transformers (all-MiniLM-L6-v2) for semantic similarity
- **Vector Store**: ChromaDB for efficient vector storage and retrieval
- **Framework**: LangChain for orchestration, Streamlit for web UI

## ğŸš€ Quick Start

### Prerequisites
- Python 3.8+
- Groq API key ([get one here](https://console.groq.com/))

### Installation

1. **Clone or download the project**
   ```bash
   cd your-project-directory
   ```

2. **Install dependencies**
   ```bash
   pip install -r requirements.txt
   ```

3. **Set up your API key**
   Create a `.env` file in the project root:
   ```bash
   GROQ_API_KEY=your-groq-api-key-here
   ```

## ğŸ¯ Usage Options

### Option 1: Web Interface (Recommended)

Launch the modern Streamlit web application:

```bash
streamlit run app.py
```

**Features:**
- Drag-and-drop PDF uploads
- Real-time chat interface
- Source document citations
- Session-based conversation memory
- No installation required for end users

**Workflow:**
1. Open the web app in your browser
2. Upload your PDF documents
3. Click "Process Documents"
4. Start asking questions!

### Option 2: Command-Line Interface

For quick testing or integration:

```bash
python rag.py
```

**Features:**
- Terminal-based interaction
- Persistent vector store
- Batch document processing

**Setup:**
- Place PDFs in the `data/` folder
- Run `python rag.py` (first run processes documents)
- Subsequent runs reuse the vector store

## ğŸ“ Project Structure

```
Simple-RAG/
â”œâ”€â”€ app.py                 # ğŸŒ Streamlit web application
â”œâ”€â”€ rag.py                 # ğŸ’» Command-line interface
â”œâ”€â”€ requirements.txt       # ğŸ“¦ Python dependencies
â”œâ”€â”€ .env                   # ğŸ”‘ API keys (create this)
â”œâ”€â”€ .env.example          # ğŸ“ API key template
â”œâ”€â”€ README.md             # ğŸ“– This file
â”œâ”€â”€ .gitignore           # ğŸš« Git ignore rules
â”œâ”€â”€ data/                 # ğŸ“„ PDF storage (for CLI)
â”‚   â””â”€â”€ README.md
â””â”€â”€ chroma_db/            # ğŸ—„ï¸ Vector store (auto-generated)
```

## ğŸ—ï¸ How It Works

### Architecture Overview

```
User Query â†’ Question Rephrasing â†’ Semantic Search â†’ Context Retrieval â†’ LLM Generation â†’ Answer
     â†“              â†“                      â†“              â†“              â†“              â†“
  "its types" â†’ "Multiclass Classification types" â†’ Vector Search â†’ Top 4 Chunks â†’ Groq Llama â†’ Final Answer
```

### Detailed Flow

1. **Document Ingestion**
   - PDFs are loaded and split into overlapping text chunks
   - Each chunk is converted to a vector embedding using Sentence Transformers
   - Embeddings are stored in ChromaDB for fast retrieval

2. **Query Processing**
   - User question is rephrased using conversation history for context
   - Query is converted to embedding and compared against document chunks
   - Top-k most similar chunks are retrieved as context

3. **Answer Generation**
   - Context chunks + rephrased question sent to Groq's Llama model
   - Model generates answer based only on provided context
   - Source documents are cited for transparency

4. **Memory Management**
   - Conversation history maintained across interactions
   - Follow-up questions automatically incorporate previous context

## âš™ï¸ Configuration

### Model Settings (in `app.py` or `rag.py`)

```python
# LLM Configuration
llm = ChatGroq(
    model="llama-3.1-8b-instant",  # Fast and cost-effective
    temperature=0.2,               # Low creativity for factual answers
)

# Retrieval Settings
retriever = vectordb.as_retriever(search_kwargs={"k": 4})  # Top 4 chunks

# Text Splitting
text_splitter = RecursiveCharacterTextSplitter(
    chunk_size=1000,     # Characters per chunk
    chunk_overlap=200,   # Overlap between chunks
)
```

### Available Models

- `llama-3.1-8b-instant` â­ (Recommended - fast and accurate)
- `llama3-8b-8192` (Legacy, may be deprecated)
- `llama3-70b-8192` (More capable but slower)
- `mixtral-8x7b-32768` (Good for complex reasoning)

## ğŸ”§ Customization

### Adding New Features

**Custom Prompts:**
```python
custom_prompt = PromptTemplate.from_template("""
You are a {role} assistant. Use the context to answer...

Context: {context}
Question: {question}
Answer:
""")
```

**Different Embeddings:**
```python
from langchain_openai import OpenAIEmbeddings
embeddings = OpenAIEmbeddings()  # Requires OpenAI API key
```

**External Vector Stores:**
```python
from langchain_pinecone import PineconeVectorStore
# For cloud-based vector storage
```

## ğŸ› Troubleshooting

### Common Issues

**âŒ "ModuleNotFoundError"**
```bash
pip install -r requirements.txt
# Or install specific package: pip install langchain-groq
```

**âŒ "GROQ_API_KEY not found"**
- Create `.env` file with: `GROQ_API_KEY=your-key-here`
- Or set environment variable: `export GROQ_API_KEY=your-key`

**âŒ "No text could be loaded from PDFs"**
- Ensure PDFs are not password-protected
- Check if PDFs contain selectable text (not just images)
- Try different PDF files

**âŒ "HuggingFace embeddings error"**
- The app uses `FakeEmbeddings` to avoid dependency issues
- For production, consider using OpenAI embeddings or fixing HF dependencies

**âŒ Streamlit app not loading**
```bash
streamlit run app.py --server.port 8501 --server.address 0.0.0.0
```

**âŒ Memory not working in conversations**
- Check if `ConversationBufferMemory` is properly initialized
- Ensure `output_key="answer"` matches your chain's output

### Performance Tips

- **Smaller chunks**: Reduce `chunk_size` for more precise retrieval
- **More context**: Increase `k` parameter for broader context
- **Caching**: Vector stores persist automatically for faster subsequent runs
- **Model selection**: Use `llama-3.1-8b-instant` for speed, larger models for complexity

## ğŸ¤ Contributing

1. Fork the repository
2. Create a feature branch: `git checkout -b feature-name`
3. Make your changes and test thoroughly
4. Submit a pull request with a clear description

### Development Setup

```bash
# Install dev dependencies
pip install -r requirements-dev.txt

# Run tests
python -m pytest

# Format code
black . && isort .
```

## ğŸ“„ License

This project is licensed under the MIT License - see the [LICENSE](LICENSE) file for details.

## ğŸ™ Acknowledgments

- **LangChain** - For the RAG framework
- **Groq** - For fast LLM inference
- **Sentence Transformers** - For semantic embeddings
- **ChromaDB** - For vector storage
- **Streamlit** - For the web interface

## ğŸ“ Support

- ğŸ“§ **Issues**: Open a GitHub issue for bugs or feature requests
- ğŸ’¬ **Discussions**: Use GitHub Discussions for questions
- ğŸ“– **Documentation**: Check this README and inline code comments

---

**Happy Learning! ğŸ“** Transform your PDFs into interactive knowledge bases with AI-powered conversations.
